# ============================================================
# Experiment: transfer-regular
# Goal: Transfer to a fixed target on d-regular random graphs.
# ============================================================

# -----------------------
# 0) Goal & task
# -----------------------
task:
  name: transfer          # {transfer | search | mixing | robust}
  target_index: -1        # g.N-1 by default (set explicitly at runtime if desired)
  reduction: mean         # mean loss across episodes
  renorm: true            # renormalize final distribution before scoring
  cvar_alpha: 0.1

# -----------------------
# 1) Graph family & sizes
# -----------------------
data:
  family: regular         # d-regular random graphs
  regular:
    N: 64                 # train size (will be swept via seeds/episodes)
    d: 3                  # node degree
  # OOD test in the paper: N=256 (handled at eval time / separate run)
  seed: 1234

# -----------------------
# 2) Horizons (can be a curriculum)
# -----------------------
sim:
  steps: [64, 96, 128]    # our train loop will pick T=max(steps)=128 by default

# -----------------------
# 3) Policy (GNN, controller, coin map)
# -----------------------
model:
  gnn:
    kind: gcn             # {gcn | gat | gin | pna}  (ACPLPolicy reads 'kind' internally)
    hidden: 128
    dropout: 0.0
  controller:
    kind: gru             # {gru | transformer}
    hidden: 128
    layers: 2
    dropout: 0.0
    bidirectional: false
  head:
    hidden: 0             # linear head
    dropout: 0.0
  time_pe_dim: 32         # learned time positional encoding dim

# Coin parameterization (any degree via Hermitian adaptor)
coin:
  family: exp             # {su2 (deg=2 only) | exp | cayley}; 'exp' supports any degree

# -----------------------
# 4) Node features
# -----------------------
features:
  degree: true
  pe:
    kind: lappe           # {lappe | none}; Laplacian PE
    K: 16
  coord_rule: none        # no absolute coords on random regular graphs

# -----------------------
# 5) Optimization
# -----------------------
train:
  epochs: 50
  batch_size: 1                 # each “episode” = one rollout on the same graph config
  episodes_per_epoch: 256       # number of episodes per epoch
  ci_n_seeds: 5                 # pooled-CI evaluation seeds
  ci_episodes: 128              # episodes per seed at CI time

optim:
  name: adam
  lr: 2.5e-4
  weight_decay: 0.0
  grad_clip:
    enabled: true
    mode: norm
    max_norm: 1.0
  precision:
    amp: auto
  scheduler:
    kind: cosine_warmup
    cosine_warmup:
      warmup_steps: 200
      t_max: 5000
      eta_min: 1.0e-5
  ema:
    enabled: true
    decay: 0.999
    warmup_steps: 100
    update_every: 1
    pin_to_device: true

# Optional param-group rules (regex on parameter names)
optim.param_groups:
  enabled: false
  rules: []

# -----------------------
# 6) Randomness
# -----------------------
seed: 0                   # master seed for this run (swept externally if needed)

# -----------------------
# 7) Baselines (not used by the trainer, kept for bookkeeping)
baselines:
  - name: hadamard_global
    desc: Fixed Hadamard coin (deg=2 graphs only; listed for completeness)
  - name: grover_global
    desc: Fixed Grover coin on regular graphs
  - name: global_time_varying
    desc: Single global coin U_t shared by all nodes, time-varying

# -----------------------
# 8) Success criteria (paper thresholds)
success:
  # Example thresholds; adjust per your report’s table.
  transfer_min_pt_target: 0.90
  ci_improvement_eps: 0.02

# -----------------------
# 9) Compute budget
# -----------------------
device: auto              # auto -> cuda if available else cpu
dtype: float32
budget:
  wallclock_hours: 4
  max_memory_gb: 8

# -----------------------
# 10) Logging
# -----------------------
log:
  backend: plain          # {plain | tensorboard | wandb}
  project: acpl-qwalk
  run_name: "${task.name}-${data.family}-N${data.regular.N}-d${data.regular.d}-T${sim.steps}"
  interval: 50
  ci_bootstrap_samples: 1000

# Where to write run artifacts (overridable via CLI: logging.dir=...)
train.run_dir: null

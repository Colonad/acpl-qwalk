# Experiment 0) Pre-registration manifest — Transfer on line graphs
goal: transfer


train:
  epochs: 50
  batch_size: 1
  episodes_per_epoch: 200
  ci_n_seeds: 5
  ci_episodes: 100


data:
  family: line                # graph family
  train_sizes: [64, 96, 128, 160, 192]
  test_ood_sizes: [256]       # out-of-distribution test size
  seed: 1234

sim:
  steps: [64, 96, 128]        # horizons (curriculum allowed)



model:
  gnn: { kind: gcn, hidden: 64 }
  controller: { kind: gru, hidden: 64, layers: 1 }
  head: { hidden: 0 }
  time_pe_dim: 32
  coin:
    family: su2
    theta_scale: 1.0
    theta_noise_std: 0.0
                

task:
  target_index: -1        # last node on the line
  loss: nll               # IMPORTANT: don't use neg_prob unless you know why
  eps: 1.0e-8
  target_radius: 3        # warm-start window: {target-3..target}
  normalize_prob: true
  reduce: mean
  cvar_alpha: 0.1



features:
  use_degree: true
  pe:
    kind: coord_1d            # simple 1D coordinate feature
    note: "LapPE optional; coord_1d used for line"

optim:
  name: adam
  lr: 3.0e-3
  weight_decay: 0.0
  grad_clip:
    enabled: true
    mode: norm
    max_norm: 1.0
  regularizers:
    temporal_smoothness: 0.0
    magnitude: 0.0
  scheduler:
    kind: none

randomness:
  master_seed: 0
  split_routing: "by_run_seed"
  seeds_per_run: 3

baselines:
  fixed_hadamard: true
  fixed_grover: false
  global_time_varying_coin: true
  analytical_skw_akr: false

success:
  transfer_threshold: 0.90     # e.g., mean P_T[target] on line >= 0.9
  ci_improvement_eps: 0.02     # pooled CI must improve by at least ε when claimed

compute:
  device: auto                 # cuda if available, else cpu
  dtype: float32
  wallclock_limit_min: 120
  max_memory_gb: 8

log:
  backend: plain
  interval: 50
  project: "acpl-experiments"
  run_name: "transfer-line"
  notes: "Pre-registered: transfer on line graphs; SU2 coins; curriculum over T."

seed: 42
device: auto
dtype: float32
state_dtype: complex64   # <-- used by train.py (NOT coin.dtype)

data:
  family: line
  N: 64                  # <-- REQUIRED for er/regular paths in your script
  seed: 1234
  directed: false

  grid:
    Lx: 8
    Ly: 8

  regular:
    d: 3

  er:
    p: 0.05

  ws:
    Lx: 8
    Ly: 8
    kx: 1
    ky: 1
    beta: 0.2
    torus: false








features:
  # ---- core structural inputs (like your GNN project) ----
  use_degree: true
  degree_onehot_K: 16        # 0 => scalar degree; 16 => one-hot bucket up to 15
  degree_log1p: false
  degree_norm: inv_sqrt      # {none, inv_sqrt, max, log}

  # ---- geometry / coordinates when present (grid/line/ws) ----
  use_coords: true
  coords_normalize: true

  # ---- task indicator channel (search/robust use this; transfer usually false) ----
  use_indicator: false

  # ---- positional encodings (the “missing piece” vs your GNN project figures) ----
  use_lap_pe: true
  lap_pe_k: 16               # must be <= N; 16 is safe for N=64
  lap_pe_norm: sym           # {sym, rw}
  lap_pe_random_sign: true   # sign ambiguity handling (deterministic via seed below)

  use_rwse: true
  rwse_K: 16                 # RWSE steps
  rwse_aggregation: diag      # keep consistent with your current encoder

  # ---- simple role encodings (helps “structured graphs” look like structured graphs) ----
  use_role_encodings: true
  role_degree_bins_K: 8
  role_include_leaf: true
  role_include_hub: true
  role_hub_percentile: 0.90
  role_include_grid_boundary: true

  # ---- off unless you're training hypercubes or explicitly want arcs-as-features ----
  use_bitstrings: false
  build_arcs: false

  # numerics / determinism
  eps: 1.0e-12
  seed: 42






sim:
  steps: 64

task:
  name: transfer
  target_index: 63
  target_radius: 0

  # train.py expects exactly: uniform | node0 | node
  init_state: node0

  # whether to append an indicator channel (search/robust typically true)
  use_indicator: false

# train.py reads loss knobs from TOP-LEVEL loss (unless task.loss is a dict)
loss:
  kind: nll
  reduction: mean
  normalize_prob: true
  eps: 1.0e-8
  margin: 0.0
  cvar_alpha: 0.1

  # mixing-only knobs (harmless for transfer)
  mix_kind: tv
  mix_eps: 1.0e-12

model:
  gnn:
    kind: gcn
    hidden: 64
    out: 64
    dropout: 0.0

  controller:
    kind: gru
    hidden: 64
    layers: 1
    dropout: 0.0
    bidirectional: false

  head:
    hidden: 0
    dropout: 0.0
    layernorm: true

  # <-- what train.py actually reads
  time_pe_dim: 32
  time_pe_learned_scale: true

coin:
  family: su2
  theta_scale: 1.0
  theta_noise_std: 0.0

optim:
  name: adam
  lr: 3.0e-3
  weight_decay: 0.0
  betas: [0.9, 0.999]
  grad_clip: 1.0

  precision:
    amp: on

  scheduler:
    kind: none

train:
  epochs: 120
  batch_size: 1
  episodes_per_epoch: 200     # <-- otherwise you silently default to 200 in code anyway
  verify_checkpoints: true

  # pooled CI block at end (optional)
  ci_n_seeds: 5
  ci_episodes: 100

log:
  backend: console
  interval: 50
  ci_bootstrap_samples: 1000
  ci_alpha: 0.05
  wandb:
    enabled: false
    project: acpl-qwalk
    run_name: A6_line64_T64_gcn_gru_su2

compile:
  enabled: false
  backend: aot_eager
  force_inductor: false
